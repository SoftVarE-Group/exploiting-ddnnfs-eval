# Empirical Evaluation on Exploitation of d-DNNFs
Replication package for FSE'22 evaluation of "Exploiting d-DNNFs for Counting-based Analysis of Feature Models"

## How to build

The python benchmark script can be used as it comes.
Furthermore, some solvers are not included in this repository due to licensing issues. 
In `solvers/` the tools are provided either as built binaries, source code, or links to respective repositories.

## How to run

For both ways you have to specify a run configuration (.json or .yaml) file or a folder that contains
one or multiple run configurations. We provide exclusively .yaml files.

### Run benchmark via Docker
TODO provide an image

You can run a container by building your own image from the Dockerfile.
Values in capital letters have to be replaced with individual values.

Build the image
```
docker build -t benchmark:TAG PATH_TO_THE_DOCKERFILE
```

Run a container with a run configuration (the following example runs all run configurations in the test directory and its subdirectories)
```
docker run --name test -it benchmark:latest-nc run_configurations/test
```

Get the result data. Here we cp the result data of our test container which we started and named in the previous step.
The DEST_PATH_ON_HOST can just be the current directory (e.g. .)
```
docker cp test:benchmark/results DEST_PATH_ON_HOST
```

### Alternative Run benchmark
In general, an experiment specified by a .json or a .yaml file can be executed with. Nonetheless, we recommend using docker

```
python3 run.py run_configurations/experiment.yaml
```

